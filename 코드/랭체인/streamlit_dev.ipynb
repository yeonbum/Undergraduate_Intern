{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b7dea-f0e6-4d89-87e6-831f1ba8ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import tempfile\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8062409-d7cd-417b-a69d-ec30bbe172c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chroma tenant ì˜¤ë¥˜ ë°©ì§€ ìœ„í•œ ì½”ë“œ\n",
    "import chromadb\n",
    "chromadb.api.client.SharedSystemClient.clear_system_cache()\n",
    "\n",
    "#ì˜¤í”ˆAI API í‚¤ ì„¤ì •\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9674efc4-69ea-4bb4-beef-d8894610611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache_resourceë¡œ í•œë²ˆ ì‹¤í–‰í•œ ê²°ê³¼ ìºì‹±í•´ë‘ê¸°\n",
    "@st.cache_resource\n",
    "def load_pdf(_file):\n",
    "    # ì„ì‹œ íŒŒì¼ì„ ìƒì„±í•´ ì—…ë¡œë“œëœ PDF íŒŒì¼ì˜ ë°ì´í„° ì €ì¥\n",
    "    with tempfile.NamedTemporaryFile(mode=\"wb\", delete=False) as tmp_file:\n",
    "        tmp_file.write(_file.getvalue())\n",
    "        tmp_file_path = tmp_file.name\n",
    "        #PDF íŒŒì¼ ì—…ë¡œë“œ\n",
    "        loader = PyPDFLoader(file_path=tmp_file_path)\n",
    "        pages = loader.load_and_split()\n",
    "    return pages\n",
    "\n",
    "#í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ Chroma ì•ˆì— ì„ë² ë”© ë²¡í„°ë¡œ ì €ì¥\n",
    "@st.cache_resource\n",
    "def create_vector_store(_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    split_docs = text_splitter.split_documents(_docs)\n",
    "    vectorstore = Chroma.from_documents(split_docs, OpenAIEmbeddings(model='text-embedding-3-small'), persist_directory=\"./upload_data\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb0cce-a219-4a3e-8875-b0f4ec1048bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ëŠ” í—¬í¼ í•¨ìˆ˜\n",
    "def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#PDF ë¬¸ì„œ ê¸°ë°˜ RAG ì²´ì¸ êµ¬ì¶•\n",
    "@st.cache_resource\n",
    "def chaining(_pages):\n",
    "    vectorstore = create_vector_store(_pages)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    qa_system_prompt = \"\"\"\n",
    "    You are an assistant for question-answering tasks. \\\n",
    "    Use the following pieces of retrieved context to answer the question. \\\n",
    "    If you don't know the answer, just say that you don't know. \\\n",
    "    Keep the answer perfect. please use imogi with the answer.\n",
    "    Please answer in Korean and use respectful language.\\\n",
    "    {context}\n",
    "    \"\"\"\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "        | qa_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a8a00-5d4f-4495-aa1b-492ea7f839d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit UI\n",
    "st.header(\"ChatPDF ğŸ’¬ ğŸ“š\")\n",
    "# Streamlitì˜ íŒŒì¼ ì—…ë¡œë“œ ê¸°ëŠ¥\n",
    "uploaded_file = st.file_uploader(\"Upload a PDF\", type=[\"pdf\"])\n",
    "if uploaded_file is not None:\n",
    "    pages = load_pdf(uploaded_file)\n",
    "\n",
    "    rag_chain = chaining(pages)\n",
    "\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!\"}]\n",
    "\n",
    "    for msg in st.session_state.messages:\n",
    "        st.chat_message(msg['role']).write(msg['content'])\n",
    "\n",
    "    if prompt_message := st.chat_input(\"ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :)\"):\n",
    "        st.chat_message(\"human\").write(prompt_message)\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt_message})\n",
    "        with st.chat_message(\"ai\"):\n",
    "            with st.spinner(\"Thinking...\"):\n",
    "                response = rag_chain.invoke(prompt_message)\n",
    "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "                st.write(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
